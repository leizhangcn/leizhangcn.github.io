<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Lei Zhang, 张磊, Homepage, IDEA, International Digital Economy Academy, 粤港澳大湾区数字经济研究院, Microsoft, 微软, Computer Vision and Robotics, 计算机视觉与机器人, Computer Vision, 计算机视觉, Deep Learning, 深度学习, Machine Learning, 机器学习">
    <meta name="author" content="Lei Zhang, 张磊">
    <meta name="keywords" content="Lei Zhang, 张磊, Homepage, IDEA, International Digital Economy Academy, 粤港澳大湾区数字经济研究院, Microsoft, 微软, Computer Vision and Robotics, 计算机视觉与机器人, Computer Vision, 计算机视觉, Deep Learning, 深度学习, Machine Learning, 机器学习">
    <link rel="shortcut icon" href="/assets/ico/favicon.ico">

    <title>Lei Zhang @ idea.edu.cn</title>

    <!-- Bootstrap core CSS -->
    <link href="/assets/css/bootstrap.min.css" rel="stylesheet">


    <!-- Custom styles for this template -->
    <link href="/assets/css/main.css" rel="stylesheet">
  </head>

  <body>
    <div class="container">
      <div class="header row">
        <div class="row">
    <div class="title pull-left">
      Lei Zhang
    </div>
    <ul class="nav nav-pills pull-right">
      
      
      <li class="active">
        <a href="/" title="">Home</a>
      </li>
      
      
      <li >
        <a href="/publications/" title="">Publications</a>
      </li>
      
    </ul>
</div>

<script>
  $()
</script>
       
      </div>
      <div class="content row">
        <div>
  <div class="img">
    <img alt="photo" src="/assets/images/leizhang.jpg">
  </div>
  <div class="text">
    
    <h1 id="lei-zhang"><strong>Lei Zhang</strong></h1>

<p>Chief Scientist<br />
Computer Vision &amp; Robotics<br />
International Digital Economy Academy (IDEA)<br />
Guangdong-HongKong-Macau Greater Bay Area<br />
Shenzhen, China</p>

<p>Email: leizhang AT idea dot edu dot cn</p>

<p><a href="https://scholar.google.com/citations?user=fIlGZToAAAAJ&amp;hl=en">Google Scholar</a> | <a href="https://www.linkedin.com/in/lei-zhang-340797/">LinkedIn</a></p>

  </div>
</div>
<div>
  
  <p>I am the Chief Scientist of Computer Vision and Robotics at International Digital Economy Academy (IDEA) and an Adjunct Professor of Hong Kong University of Science and Technology (Guangzhou). Prior to this, I was a Principal Researcher and Research Manager at Microsoft, where I have worked since 2001 in Microsoft Research Asia (MSRA) for 12 years and later joined Bing Multimedia, Microsoft Research (MSR, Redmond), and Microsoft Cloud &amp; AI from 2013 to 2021. My research interests are in computer vision and machine learning. I am particularly intersted in generic visual recognition at large scale and was named as IEEE Fellow for my contributions in this area.</p>

<p>I have served as editorial board members for IEEE T-MM, T-CSVT, and Multimedia System Journal, as program co-chairs, area chairs, or committee members for many top conferences. I have published 150+ papers and hold 60+ US patents.</p>

<p>I received all my degrees (B.E., M.E., and Ph.D) in Computer Science from Tsinghua University.</p>


  <hr>
  <h1>
    Openings
  </h1>
  
  <p>Our team has made significant contributions to object detection, including <a href="https://github.com/IDEA-Research/DINO">DINO</a>, <a href="https://github.com/IDEA-Research/detrex">detrex</a>, <a href="https://github.com/IDEA-Research/groundingdino">Grounding DINO</a>, <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">Grounded SAM</a>, <a href="https://www.deepdataspace.com/blog/T-Rex">T-Rex2</a>, and <a href="https://www.deepdataspace.com/blog/Grounding-DINO-1.5-Pro">Grounding DINO 1.5</a>.</p>

<ul>
  <li>
    <p>We are hiring both <strong>researchers and engineers</strong> working on computer vision and machine learning. Note that we don’t have a clear boundary between these two roles. Most of our high impact projects are the result of team work with both researchers and engineers. Therefore, we expect our researchers to have solid engineering skills and our engineers to have good knowledge on research algorithms.</p>
  </li>
  <li>
    <p>We are also looking for <strong>research and engineering interns</strong>. In particular, we currently look for interns who have good experience on model architecture design, optimization, and compression, and are interested in deploying vision models to resource-constrained hardware devices.</p>
  </li>
  <li>
    <p>I normally recruit one <strong>PhD student</strong> each year at SCUT and start to recruit 1-2 PhD students from 2024 at HKUST-GZ in the general area of computer vision, including object detection, segmentation, open-vocabulary recognition, multi-modality understanding, contention generation, etc.</p>
  </li>
</ul>

<p>If you are interested, please send me your resume with subject “Job application in IDEA” or “Intern application in IDEA” or “PhD application”. I won’t be able to reply each individual email. But if I find your experience a good fit to our team, I will get back.</p>

<p>For all positions, we value your core skills. If you’ve done any interesting research or made extensive contributions to any open-source project, highlight that in your resume and inquiry email!</p>


  <hr>
  <h1>
    Project Highlight
  </h1>
  
  <h2 id="grounding-dino-15-advance-the-edge-of-open-set-object-detection"><strong>Grounding DINO 1.5</strong>: Advance the “Edge” of Open-Set Object Detection</h2>
<p style="text-align: center;">
<video width="360" x5-video-player-type="h5-page" x5-playsinline="playsinline" playsinline="" webkit-playsinline="true" webkit-presentation-mode="inline" x-webkit-airplay="true" autoplay="" loop="" controls="" muted="" node="[object Object]" poster="https://dds-frontend.oss-accelerate.aliyuncs.com/static_files/video/banner_grounding-dino_1.5_pro_20240515.mp4?x-oss-process=video/snapshot,t_0,m_fast"> <source src="https://dds-frontend.oss-accelerate.aliyuncs.com/static_files/video/banner_grounding-dino_1.5_pro_20240515.mp4" type="video/mp4" />  Your browser does not support the video tag. </video>  
</p>
<p style="text-align: center;">
Grounding DINO 1.5 Pro
</p>

<p style="text-align: center;">
<video width="360" x5-video-player-type="h5-page" x5-playsinline="playsinline" playsinline="" webkit-playsinline="true" webkit-presentation-mode="inline" x-webkit-airplay="true" autoplay="" loop="" controls="" muted="" node="[object Object]" poster="https://dds-frontend.oss-accelerate.aliyuncs.com/static_files/video/banner_video_edge_20240514.mp4?x-oss-process=video/snapshot,t_0,m_fast"> <source src="https://dds-frontend.oss-accelerate.aliyuncs.com/static_files/video/banner_video_edge_20240514.mp4" type="video/mp4" />Your browser does not support the video tag.</video>  
</p>
<p style="text-align: center;">
Grounding DINO 1.5 Edge  
</p>

<p>Grounding DINO 1.5 improves <a href="https://github.com/IDEA-Research/groundingdino">Grounding DINO</a> with two distinct models. The Pro model aims to push the boundary of open-set object detection, setting new records on zero-shot detection benchmarks: 54.3 AP on COCO, 55.7 AP on LVIS, and 30.2 AP on ODinW. The Edge model is optimized for speed, achieving 75.2 FPS on NVIDIA A100 and over 10 FPS on NVIDIA Orin NX while maintaining an excellent balance between efficiency and accuracy.</p>

<p>[ <a href="https://arxiv.org/abs/2405.10300">Paper</a> | <a href="https://www.deepdataspace.com/blog/Grounding-DINO-1.5-Pro">Blog</a> | <a href="https://deepdataspace.com/playground/grounding_dino">Playground</a> ]</p>

<hr style="width:75%" />

<h2 id="t-rex2-towards-generic-object-detection-via-text-visual-prompt-synergy"><strong>T-Rex2</strong>: Towards Generic Object Detection via Text-Visual Prompt Synergy</h2>

<p style="text-align: center;">
<video width="360" x5-video-player-type="h5-page" x5-playsinline="playsinline" playsinline="" webkit-playsinline="true" webkit-presentation-mode="inline" x-webkit-airplay="true" autoplay="" loop="" controls="" muted="" node="[object Object]" poster="https://dds-frontend.oss-accelerate.aliyuncs.com/static_files/video/whatsnext.mp4?x-oss-process=video/snapshot,t_0,m_fast"> <source src="https://dds-frontend.oss-accelerate.aliyuncs.com/static_files/video/whatsnext.mp4" type="video/mp4" />Your browser does not support the video tag.</video>
</p>

<p>T-Rex2 uniquely combines text and visual prompts for open-set object detection. In particular, its visual prompt capability can unlock a wide range of scenarios without the need for task-specific tuning or extensive training datasets.</p>

<p>[ <a href="https://arxiv.org/abs/2403.14610">Paper</a> | <a href="https://www.deepdataspace.com/blog/T-Rex">Blog</a> |  <a href="https://www.deepdataspace.com/playground/ivp">Playground</a> ]</p>


  <hr>
  <h1>
    Recent Publications
  </h1>
  
  <ol class="bibliography"><li><div class="pub row">
  <div class="pub-image col-md-3">
    
    <img src="/assets/pdf/yang2024openworld.jpg" >
    
  </div>
  <div class="col-md-9">
    <span id="yang2024openworld"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>Open-World Human-Object Interaction Detection via Multi-modal Prompts</b></div><div class="csl-block csl-author">Jie Yang, Bingliang Li, Ailing Zeng, <b>Lei Zhang</b> and Ruimao Zhang</div><div class="csl-block csl-event"><i>Computer Vision and Pattern Recognition (CVPR)</i>, 2024</div></div></span>

    <span id="yang2024openworld_materials">
      <ul class="tag">
        
    
        <span><a class="bib-materials" data-target="#yang2024openworld_bibtex" data-toggle="collapse" href="#yang2024openworld" onclick="return false" aria-expanded="false">BibTex</a></span>
    
    
        
          
          <span>
            <a class="bib-materials" href="https://arxiv.org/abs/2305.12252" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'yang2024openworld']);">arXiv</a>
            (<a class="bib-materials" href="https://arxiv.org/pdf/2305.12252.pdf" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'yang2024openworld']);">pdf</a>)
          </span>
        


        

      </ul>
    
      
    
      <pre id="yang2024openworld_bibtex" class="pre pre-scrollable collapse">@inproceedings{yang2024openworld,
  title = {Open-World Human-Object Interaction Detection via Multi-modal Prompts},
  author = {Yang, Jie and Li, Bingliang and Zeng, Ailing and Zhang, Lei and Zhang, Ruimao},
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  year = {2024}
}
</pre>
    
    </span>
  </div>
</div>
</li>
<li><div class="pub row">
  <div class="pub-image col-md-3">
    
    <img src="/assets/pdf/li2024visual.jpg" >
    
  </div>
  <div class="col-md-9">
    <span id="li2024visual"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>Visual In-Context Prompting</b></div><div class="csl-block csl-author">Feng Li, Qing Jiang, Hao Zhang, Shilong Liu, Huaizhe Xu, Xueyan Zou, Tianhe Ren, Hongyang Li, <b>Lei Zhang</b>, Chunyuan Li, Jianwei Yang and Jianfeng Gao</div><div class="csl-block csl-event"><i>Computer Vision and Pattern Recognition (CVPR)</i>, 2024</div></div></span>

    <span id="li2024visual_materials">
      <ul class="tag">
        
    
        <span><a class="bib-materials" data-target="#li2024visual_bibtex" data-toggle="collapse" href="#li2024visual" onclick="return false" aria-expanded="false">BibTex</a></span>
    
    
        
          
          <span>
            <a class="bib-materials" href="https://arxiv.org/abs/2311.13601" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'li2024visual']);">arXiv</a>
            (<a class="bib-materials" href="https://arxiv.org/pdf/2311.13601.pdf" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'li2024visual']);">pdf</a>)
          </span>
        


        
        <span><a class="bib-materials" href="https://github.com/UX-Decoder/DINOv">Code</a></span>
        

      </ul>
    
      
    
      <pre id="li2024visual_bibtex" class="pre pre-scrollable collapse">@inproceedings{li2024visual,
  title = {Visual In-Context Prompting},
  author = {Li, Feng and Jiang, Qing and Zhang, Hao and Liu, Shilong and Xu, Huaizhe and Zou, Xueyan and Ren, Tianhe and Li, Hongyang and Zhang, Lei and Li, Chunyuan and Yang, Jianwei and Gao, Jianfeng},
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  year = {2024}
}
</pre>
    
    </span>
  </div>
</div>
</li>
<li><div class="pub row">
  <div class="pub-image col-md-3">
    
    <img src="/assets/pdf/huang2024tag.jpg" >
    
  </div>
  <div class="col-md-9">
    <span id="huang2024tag"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>Tag2Text: Guiding Vision-Language Model via Image Tagging</b></div><div class="csl-block csl-author">Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo and <b>Lei Zhang</b></div><div class="csl-block csl-event"><i>International Conference on Learning Representations (ICLR)</i>, 2024</div></div></span>

    <span id="huang2024tag_materials">
      <ul class="tag">
        
    
        <span><a class="bib-materials" data-target="#huang2024tag_bibtex" data-toggle="collapse" href="#huang2024tag" onclick="return false" aria-expanded="false">BibTex</a></span>
    
    
        
          
          <span>
            <a class="bib-materials" href="https://arxiv.org/abs/2303.05657" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'huang2024tag']);">arXiv</a>
            (<a class="bib-materials" href="https://arxiv.org/pdf/2303.05657.pdf" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'huang2024tag']);">pdf</a>)
          </span>
        


        
        <span><a class="bib-materials" href="https://github.com/xinyu1205/recognize-anything">Code</a></span>
        

      </ul>
    
      
    
      <pre id="huang2024tag_bibtex" class="pre pre-scrollable collapse">@inproceedings{huang2024tag,
  title = {Tag2Text: Guiding Vision-Language Model via Image Tagging},
  author = {Huang, Xinyu and Zhang, Youcai and Ma, Jinyu and Tian, Weiwei and Feng, Rui and Zhang, Yuejie and Li, Yaqian and Guo, Yandong and Zhang, Lei},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2024}
}
</pre>
    
    </span>
  </div>
</div>
</li>
<li><div class="pub row">
  <div class="pub-image col-md-3">
    
    <img src="/assets/pdf/liu2024symbol.jpg" >
    
  </div>
  <div class="col-md-9">
    <span id="liu2024symbol"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>Symbol as Points: Panoptic Symbol Spotting via Point-based Representation</b></div><div class="csl-block csl-author">Wenlong Liu, Tianyu Yang, Yuhan Wang, Qizhi Yu and <b>Lei Zhang</b></div><div class="csl-block csl-event"><i>International Conference on Learning Representations (ICLR)</i>, 2024</div></div></span>

    <span id="liu2024symbol_materials">
      <ul class="tag">
        
    
        <span><a class="bib-materials" data-target="#liu2024symbol_bibtex" data-toggle="collapse" href="#liu2024symbol" onclick="return false" aria-expanded="false">BibTex</a></span>
    
    
        
          
          <span>
            <a class="bib-materials" href="https://arxiv.org/abs/2401.10556" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'liu2024symbol']);">arXiv</a>
            (<a class="bib-materials" href="https://arxiv.org/pdf/2401.10556.pdf" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'liu2024symbol']);">pdf</a>)
          </span>
        


        
        <span><a class="bib-materials" href="https://github.com/nicehuster/SymPoint">Code</a></span>
        

      </ul>
    
      
    
      <pre id="liu2024symbol_bibtex" class="pre pre-scrollable collapse">@inproceedings{liu2024symbol,
  title = {Symbol as Points: Panoptic Symbol Spotting via Point-based Representation},
  author = {Liu, Wenlong and Yang, Tianyu and Wang, Yuhan and Yu, Qizhi and Zhang, Lei},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2024}
}
</pre>
    
    </span>
  </div>
</div>
</li>
<li><div class="pub row">
  <div class="pub-image col-md-3">
    
    <img src="/assets/pdf/cheng2024progressive.jpg" >
    
  </div>
  <div class="col-md-9">
    <span id="cheng2024progressive"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts</b></div><div class="csl-block csl-author">Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, <b>Lei Zhang</b>, Jian Zhang and Li Yuan</div><div class="csl-block csl-event"><i>International Conference on Learning Representations (ICLR)</i>, 2024</div></div></span>

    <span id="cheng2024progressive_materials">
      <ul class="tag">
        
    
        <span><a class="bib-materials" data-target="#cheng2024progressive_bibtex" data-toggle="collapse" href="#cheng2024progressive" onclick="return false" aria-expanded="false">BibTex</a></span>
    
    
        
          
          <span>
            <a class="bib-materials" href="https://arxiv.org/abs/2310.11784" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'cheng2024progressive']);">arXiv</a>
            (<a class="bib-materials" href="https://arxiv.org/pdf/2310.11784.pdf" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'cheng2024progressive']);">pdf</a>)
          </span>
        


        
        <span><a class="bib-materials" href="https://github.com/cxh0519/Progressive3D">Code</a></span>
        

      </ul>
    
      
    
      <pre id="cheng2024progressive_bibtex" class="pre pre-scrollable collapse">@inproceedings{cheng2024progressive,
  title = {Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts},
  author = {Cheng, Xinhua and Yang, Tianyu and Wang, Jianan and Li, Yu and Zhang, Lei and Zhang, Jian and Yuan, Li},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2024}
}
</pre>
    
    </span>
  </div>
</div>
</li>
<li><div class="pub row">
  <div class="pub-image col-md-3">
    
    <img src="/assets/pdf/huang2024dreamtime.jpg" >
    
  </div>
  <div class="col-md-9">
    <span id="huang2024dreamtime"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation</b></div><div class="csl-block csl-author">Yukun Huang, Jianan Wang, Yukai Shi, Boshi Tang, Xianbiao Qi and <b>Lei Zhang</b></div><div class="csl-block csl-event"><i>International Conference on Learning Representations (ICLR)</i>, 2024</div></div></span>

    <span id="huang2024dreamtime_materials">
      <ul class="tag">
        
    
        <span><a class="bib-materials" data-target="#huang2024dreamtime_bibtex" data-toggle="collapse" href="#huang2024dreamtime" onclick="return false" aria-expanded="false">BibTex</a></span>
    
    
        
          
          <span>
            <a class="bib-materials" href="https://arxiv.org/abs/2306.12422" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'huang2024dreamtime']);">arXiv</a>
            (<a class="bib-materials" href="https://arxiv.org/pdf/2306.12422.pdf" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'huang2024dreamtime']);">pdf</a>)
          </span>
        


        

      </ul>
    
      
    
      <pre id="huang2024dreamtime_bibtex" class="pre pre-scrollable collapse">@inproceedings{huang2024dreamtime,
  title = {DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation},
  author = {Huang, Yukun and Wang, Jianan and Shi, Yukai and Tang, Boshi and Qi, Xianbiao and Zhang, Lei},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2024}
}
</pre>
    
    </span>
  </div>
</div>
</li>
<li><div class="pub row">
  <div class="pub-image col-md-3">
    
    <img src="/assets/pdf/shi2024toss.jpg" >
    
  </div>
  <div class="col-md-9">
    <span id="shi2024toss"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>TOSS: High-quality Text-guided Novel View Synthesis from a Single Image</b></div><div class="csl-block csl-author">Yukai Shi, Jianan Wang, He CAO, Boshi Tang, Xianbiao Qi, Tianyu Yang, Yukun Huang, Shilong Liu, <b>Lei Zhang</b> and Heung-Yeung Shum</div><div class="csl-block csl-event"><i>International Conference on Learning Representations (ICLR)</i>, 2024</div></div></span>

    <span id="shi2024toss_materials">
      <ul class="tag">
        
    
        <span><a class="bib-materials" data-target="#shi2024toss_bibtex" data-toggle="collapse" href="#shi2024toss" onclick="return false" aria-expanded="false">BibTex</a></span>
    
    
        
          
          <span>
            <a class="bib-materials" href="https://arxiv.org/abs/2310.10644" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'shi2024toss']);">arXiv</a>
            (<a class="bib-materials" href="https://arxiv.org/pdf/2310.10644.pdf" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'shi2024toss']);">pdf</a>)
          </span>
        


        
        <span><a class="bib-materials" href="https://github.com/IDEA-Research/TOSS">Code</a></span>
        

      </ul>
    
      
    
      <pre id="shi2024toss_bibtex" class="pre pre-scrollable collapse">@inproceedings{shi2024toss,
  title = {TOSS: High-quality Text-guided Novel View Synthesis from a Single Image},
  author = {Shi, Yukai and Wang, Jianan and CAO, He and Tang, Boshi and Qi, Xianbiao and Yang, Tianyu and Huang, Yukun and Liu, Shilong and Zhang, Lei and Shum, Heung-Yeung},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2024}
}
</pre>
    
    </span>
  </div>
</div>
</li>
<li><div class="pub row">
  <div class="pub-image col-md-3">
    
    <img src="/assets/pdf/huang2023dreamwaltz.jpg" >
    
  </div>
  <div class="col-md-9">
    <span id="huang2023dreamwaltz"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>DreamWaltz: Make a Scene with Complex 3D Animatable Avatars</b></div><div class="csl-block csl-author">Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao Qi, Yukai Shi, Zheng-Jun Zha and <b>Lei Zhang</b></div><div class="csl-block csl-event"><i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2023</div></div></span>

    <span id="huang2023dreamwaltz_materials">
      <ul class="tag">
        
    
        <span><a class="bib-materials" data-target="#huang2023dreamwaltz_bibtex" data-toggle="collapse" href="#huang2023dreamwaltz" onclick="return false" aria-expanded="false">BibTex</a></span>
    
    
        
          
          <span>
            <a class="bib-materials" href="https://arxiv.org/abs/2305.12529" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'huang2023dreamwaltz']);">arXiv</a>
            (<a class="bib-materials" href="https://arxiv.org/pdf/2305.12529.pdf" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'huang2023dreamwaltz']);">pdf</a>)
          </span>
        


        
        <span><a class="bib-materials" href="https://github.com/IDEA-Research/DreamWaltz">Code</a></span>
        

      </ul>
    
      
    
      <pre id="huang2023dreamwaltz_bibtex" class="pre pre-scrollable collapse">@inproceedings{huang2023dreamwaltz,
  title = {DreamWaltz: Make a Scene with Complex 3D Animatable Avatars},
  author = {Huang, Yukun and Wang, Jianan and Zeng, Ailing and Cao, He and Qi, Xianbiao and Shi, Yukai and Zha, Zheng-Jun and Zhang, Lei},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2023}
}
</pre>
    
    </span>
  </div>
</div>
</li>
<li><div class="pub row">
  <div class="pub-image col-md-3">
    
    <img src="/assets/pdf/cai2023smplerx.jpg" >
    
  </div>
  <div class="col-md-9">
    <span id="cai2023smplerx"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation</b></div><div class="csl-block csl-author">Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Yanjun Wang, Hui En Pang, Haiyi Mei, Mingyuan Zhang, <b>Lei Zhang</b>, Chen Change Loy, Lei Yang and Ziwei Liu</div><div class="csl-block csl-event"><i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2023</div></div></span>

    <span id="cai2023smplerx_materials">
      <ul class="tag">
        
    
        <span><a class="bib-materials" data-target="#cai2023smplerx_bibtex" data-toggle="collapse" href="#cai2023smplerx" onclick="return false" aria-expanded="false">BibTex</a></span>
    
    
        
          
          <span>
            <a class="bib-materials" href="https://arxiv.org/abs/2309.17448" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'cai2023smplerx']);">arXiv</a>
            (<a class="bib-materials" href="https://arxiv.org/pdf/2309.17448.pdf" onClick="_gaq.push(['_trackEvent', 'Download', 'arXiv', 'cai2023smplerx']);">pdf</a>)
          </span>
        


        
        <span><a class="bib-materials" href="https://github.com/caizhongang/SMPLer-X">Code</a></span>
        

      </ul>
    
      
    
      <pre id="cai2023smplerx_bibtex" class="pre pre-scrollable collapse">@inproceedings{cai2023smplerx,
  title = {SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation},
  author = {Cai, Zhongang and Yin, Wanqi and Zeng, Ailing and Wei, Chen and Sun, Qingping and Wang, Yanjun and Pang, Hui En and Mei, Haiyi and Zhang, Mingyuan and Zhang, Lei and Loy, Chen Change and Yang, Lei and Liu, Ziwei},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2023}
}
</pre>
    
    </span>
  </div>
</div>
</li>
<li><div class="pub row">
  <div class="pub-image col-md-3">
    
    <img src="/assets/pdf/liu2023comprehensive.jpg" >
    
  </div>
  <div class="col-md-9">
    <span id="liu2023comprehensive"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>A Comprehensive Benchmark for Neural Human Body Rendering</b></div><div class="csl-block csl-author">Kenkun Liu, Derong Jin, Ailing Zeng, Xiaoguang Han and <b>Lei Zhang</b></div><div class="csl-block csl-event"><i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2023</div></div></span>

    <span id="liu2023comprehensive_materials">
      <ul class="tag">
        
    
        <span><a class="bib-materials" data-target="#liu2023comprehensive_bibtex" data-toggle="collapse" href="#liu2023comprehensive" onclick="return false" aria-expanded="false">BibTex</a></span>
    
    
        
          <span><a class="bib-materials" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/6e566c91d381bd7a45647d9a90838817-Paper-Datasets_and_Benchmarks.pdf" onClick="_gaq.push(['_trackEvent', 'Download', 'PDF', 'liu2023comprehensive']);">pdf</a></span>
        


        

      </ul>
    
      
    
      <pre id="liu2023comprehensive_bibtex" class="pre pre-scrollable collapse">@inproceedings{liu2023comprehensive,
  title = {A Comprehensive Benchmark for Neural Human Body Rendering},
  author = {Liu, Kenkun and Jin, Derong and Zeng, Ailing and Han, Xiaoguang and Zhang, Lei},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2023}
}
</pre>
    
    </span>
  </div>
</div>
</li></ol>

</div>

      </div>
      <div class="footer row">
        <p>
    <div class="copyright">
        &copy; <a href="http://leizhang.org">leizhang</a> 2021, 
        built using 
        <a href="http://jekyllrb.com">jekyll</a>, 
        <a href="https://github.com/inukshuk/jekyll-scholar">jekyll-scholar</a> and 
        <a href="http://getbootstrap.com">bootstrap</a>. 
    </div>
    <div class="counter">
        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=owLczor8SEq0EmUvoXG2ATi3o_GLTsQgtP6-uNt7UoI&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'></script>
    </div>
</p>        
      </div>
    </div>
    <script src="https://code.jquery.com/jquery.js"></script>
    <script src="/assets/js/bootstrap.min.js"></script>
  </body>
</html>
